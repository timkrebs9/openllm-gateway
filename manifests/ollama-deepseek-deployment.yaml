# filepath: c:\Development\openllm-gateway\manifests\ollama-deepseek-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-deepseek
  labels:
    app: ollama-deepseek
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama-deepseek
  template:
    metadata:
      labels:
        app: ollama-deepseek
    spec:
      containers:
      - name: ollama
        image: ollama/ollama:latest
        # Ollama automatically downloads models on first request if not present.
        # The deepseek-r1:8b model will be pulled when the API first calls it.
        ports:
        - containerPort: 11434
          name: api
        volumeMounts:
        - name: ollama-data
          mountPath: /root/.ollama # Default Ollama data directory
        resources: # Optional: Define resource requests/limits
          requests:
            memory: "8Gi" # Adjust based on model size and expected load
            cpu: "2"      # Adjust based on expected load
          limits:
            memory: "16Gi"
            cpu: "4"
      volumes:
      - name: ollama-data
        persistentVolumeClaim:
          claimName: ollama-deepseek-data-pvc # Use the PVC defined above
      # If not using PVC, use emptyDir (data lost on pod restart):
      # volumes:
      # - name: ollama-data
      #   emptyDir: {}

---
apiVersion: v1
kind: Service
metadata:
  name: ollama-deepseek # Service name used by the FastAPI app
spec:
  selector:
    app: ollama-deepseek
  ports:
    - protocol: TCP
      port: 11434 # Port the service listens on
      targetPort: api # Port on the pod (containerPort name)
  type: ClusterIP # Only needs to be accessible within the cluster